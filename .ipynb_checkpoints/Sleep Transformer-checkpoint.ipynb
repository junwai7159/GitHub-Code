{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e54d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b77911d",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b90b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data as list\n",
    "\n",
    "data_features_list = []\n",
    "data_labels_list = []\n",
    "\n",
    "for i in range(32):\n",
    "    filename = 'C:/Users/User/OneDrive - sjtu.edu.cn/SJTU/Y3-2/PRP/PRP Transformer/data/datapre{}-0.npz'.format(i+1)\n",
    "    data = np.load(filename, allow_pickle=True)\n",
    "    data = data['datapre']\n",
    "\n",
    "    # Features\n",
    "    data_feature = data[:, [0, 1, 7, 8]]\n",
    "    data_feature = torch.tensor(data_feature, dtype=torch.float32)\n",
    "#     data_feature = data_feature[:350]\n",
    "    data_features_list.append(data_feature)\n",
    "\n",
    "    # Labels\n",
    "    #0清醒，1浅睡N1阶段，2浅睡N2阶段，3深睡阶段，4REM期，我一般做的时候都把1和2合并成一类，都变成2，这样我们就一共有，0,2,3,4共四类\n",
    "    data_label = data[:, -1]\n",
    "    data_label = data_label.astype(int)\n",
    "    data_label[data_label == 1] = 2\n",
    "    data_label[data_label == 5] = 4\n",
    "    data_label[data_label != 0] -= 1\n",
    "    data_label = torch.tensor(data_label, dtype=torch.int64)\n",
    "    data_label = F.one_hot(data_label)\n",
    "    data_label = data_label.type(torch.float32)\n",
    "#     data_label = data_label[:350]\n",
    "    data_labels_list.append(data_label)\n",
    "    \n",
    "\n",
    "# train test split. Size of train data is 80% and size of test data is 20%. \n",
    "data_feature_train, data_feature_test, data_label_train, data_label_test = train_test_split(data_features_list,\n",
    "                                                                             data_labels_list,\n",
    "                                                                             test_size = 0.2,\n",
    "                                                                             random_state = 42)\n",
    "data_feature_train = torch.stack(data_feature_train)\n",
    "data_feature_test = torch.stack(data_feature_test)\n",
    "data_label_train = torch.stack(data_label_train)\n",
    "data_label_test = torch.stack(data_label_test)\n",
    "\n",
    "data_feature_train.shape, data_feature_test.shape, data_label_train.shape, data_label_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6949a0c4",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, epoch and iteration\n",
    "batch_size = 1\n",
    "n_iters = 8000\n",
    "num_epochs = n_iters / (len(data_feature_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_dataset = TensorDataset(data_feature_train, data_label_train)\n",
    "test_dataset = TensorDataset(data_feature_test, data_label_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for batch_features, batch_targets in train_loader:\n",
    "    print(batch_features.shape, batch_targets.shape)\n",
    "\n",
    "# batch_features: (batch_size, feature_size); batch_targets: (batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c2f1e",
   "metadata": {},
   "source": [
    "### Many-to-many RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=256, layer_dim=2, output_dim=4, batch_first=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim, num_layers=layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        # One time step\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        fc_output = self.fc(out)\n",
    "        fc_output = fc_output.softmax(dim=2)\n",
    "        \n",
    "        return fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN\n",
    "input_dim = 4    # input dimension\n",
    "hidden_dim = 256  # hidden layer dimension\n",
    "layer_dim = 2     # number of hidden layers\n",
    "output_dim = 4   # output dimension\n",
    "# seq_dim = 350\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Cross Entropy Loss \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5592f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "count = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, labels) in enumerate(train_loader):\n",
    "        seq_dim=x.shape[1]; print(i)\n",
    "        train  = Variable(x.view(-1, seq_dim, input_dim)) # (batch_size, seq_len, features)\n",
    "        labels = Variable(labels)\n",
    "            \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(train)\n",
    "    \n",
    "        # Calculate softmax and ross entropy loss\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if count % 250 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for x, labels in test_loader:\n",
    "                \n",
    "                # Forward propagation\n",
    "                test  = Variable(x.view(-1, seq_dim, input_dim))\n",
    "                outputs = model(test)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.argmax(outputs.data, dim=2)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(1)\n",
    "                correct += (predicted == torch.argmax(labels, dim=2)).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            if count % 500 == 0:\n",
    "                # Print Loss\n",
    "                print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc63a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization loss \n",
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"RNN: Loss vs Number of iteration\")\n",
    "plt.show()\n",
    "\n",
    "# visualization accuracy \n",
    "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"RNN: Accuracy vs Number of iteration\")\n",
    "plt.savefig('graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f82c1b",
   "metadata": {},
   "source": [
    "### Time-Series Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "    input_size: int,                      # number of input variables. 1 if univariate\n",
    "    dec_seq_len: int,                     # length of the input sequence fed to the decoder\n",
    "    batch_first: bool=False,\n",
    "    out_seq_len: int=58,                  \n",
    "    dim_val: int=512,                     # d_model\n",
    "    n_encoder_layers: int=4,              # number of stacked encoder layers\n",
    "    n_decoder_layers: int=4,              # number of stacked decoder layers\n",
    "    n_heads: int=8,                       # number of attention heads\n",
    "    dropout_encoder: float=0.2,           # dropout rate of encoder\n",
    "    dropout_decoder: float=0.2,           # dropout rate of decoder\n",
    "    dropout_pos_enc: float=0.1,           # dropout rate of positional encoder\n",
    "    dim_feedforward_encoder: int=2048,    # number of neurons in the linear layer of encoder\n",
    "    dim_feedforward_decoder: int=2048,    # number of neurons in the linear layer of encoder\n",
    "    num_predicted_features: int=1         # number of features to be predicted\n",
    "    ): \n",
    "        \n",
    "        super().__init__()\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "        \n",
    "        # Three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(in_features=input_size, out_features=dim_val)\n",
    "        self.linear_mapping = nn.Linear(in_features=dim_val, out_features=num_predicted_features)\n",
    "        \n",
    "        # Positional encoder\n",
    "        self.positional_encoding_layer = pe.PositionalEncoder(d_model=dim_val, dropout=dropout_pos_enc)\n",
    "        \n",
    "        # Stack the encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_val, nhead=n_heads, dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout_encoder, batch_first=batch_first\n",
    "            )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer, num_layers=n_encoder_layers, norm=None\n",
    "            )\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = self.encoder_input_layer(src)\n",
    "        src = self.positional_encoding_layer(src)\n",
    "        src = self.encoder(src=src)\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833b290",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303fe4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input_size = len(torch.unique(torch.argmax(data_label, dim=1)))\n",
    "# dec_seq_len = len(data_feature)\n",
    "\n",
    "# model = TimeSeriesTransformer(input_size=input_size, dec_seq_len=dec_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_none = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "loss_func_mean = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "loss_func_sum = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "pre = torch.tensor([[0.8, 0.5, 0.2, 0.5],\n",
    "                    [0.2, 0.9, 0.3, 0.2],\n",
    "                    [0.4, 0.3, 0.7, 0.1],\n",
    "                    [0.1, 0.2, 0.4, 0.8]], dtype=torch.float)\n",
    "tgt = torch.tensor([[1, 0, 0, 0],\n",
    "                    [0, 1, 0, 0],\n",
    "                    [0, 0, 1, 0],\n",
    "                    [0, 0, 0, 1]], dtype=torch.float)\n",
    "print(loss_func_none(pre, tgt))\n",
    "print(loss_func_mean(pre, tgt))\n",
    "print(loss_func_sum(pre, tgt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
